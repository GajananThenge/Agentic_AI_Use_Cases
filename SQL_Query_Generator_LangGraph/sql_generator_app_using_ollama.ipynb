{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StateGraph' from 'langchain_core.runnables.graph' (/Users/A200105820/Documents/Gen AI Use Cases/.conda/lib/python3.12/site-packages/langchain_core/runnables/graph.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Runnable\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph, START, END\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhistory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MemorySaver\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hub\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'StateGraph' from 'langchain_core.runnables.graph' (/Users/A200105820/Documents/Gen AI Use Cases/.conda/lib/python3.12/site-packages/langchain_core/runnables/graph.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from typing_extensions import Annotated\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
    "\n",
    "from langgraph.graph import START, StateGraph,END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.chat_models.ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"ollama/deepseek-r1:8b\")  #\"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/A200105820/Documents/Gen AI Use Cases'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'AntÃ´nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:////Users/A200105820/Documents/Gen AI Use Cases/SQL_Query_Generator_LangGraph/Chinook.db\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM Artist LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def query_template():\n",
    "    query_prompt_template = hub.pull(\"langchain-ai/sql-query-system-prompt\")\n",
    "    return query_prompt_template\n",
    "    #query_prompt_template.messages[0].pretty_print()\n",
    "    \n",
    "def my_llm():\n",
    "    #llm = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\",temperature=0)\n",
    "    llm = ChatOllama(\n",
    "        model=\"ollama/deepseek-r1:8b\",\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    )\n",
    "    \n",
    "    return llm\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: str\n",
    "    result: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "\n",
    "\n",
    "def write_query(state: State):\n",
    "\n",
    "    query_prompt_template= query_template()\n",
    "    \n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "    prompt = query_prompt_template.invoke(\n",
    "        {\n",
    "            \"dialect\": db.dialect,\n",
    "            \"top_k\": 10,\n",
    "            \"table_info\": db.get_table_info(),\n",
    "            \"input\": state[\"question\"],\n",
    "        }\n",
    "    )\n",
    "    llm = my_llm()\n",
    "    structured_llm = llm.with_structured_output(QueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    return {\"query\": result[\"query\"]}\n",
    "\n",
    "\n",
    "def execute_query(state: State):\n",
    "    \n",
    "    \"\"\"Execute SQL query.\"\"\"\n",
    "    execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
    "    return {\"result\": execute_query_tool.invoke(state[\"query\"])}\n",
    "\n",
    "\n",
    "def generate_answer(state: State):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, \"\n",
    "        \"and SQL result, answer the user question.\\n\\n\"\n",
    "        f'Question: {state[\"question\"]}\\n'\n",
    "        f'SQL Query: {state[\"query\"]}\\n'\n",
    "        f'SQL Result: {state[\"result\"]}'\n",
    "    )\n",
    "    llm = my_llm()\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def analyst(state: State):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, \"\n",
    "        \"and SQL result, answer the user question.\\n\\n\"\n",
    "        f'Question: {state[\"question\"]}\\n'\n",
    "        f'SQL Query: {state[\"query\"]}\\n'\n",
    "        f'SQL Result: {state[\"result\"]}'\n",
    "    )\n",
    "    llm = my_llm()\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def create_sql_graph():\n",
    "    # Initialize the StateGraph with the State TypedDict\n",
    "    graph_builder = StateGraph(State).add_sequence(\n",
    "        [write_query, execute_query, generate_answer]\n",
    "    )\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    graph_builder.add_edge(START, \"write_query\")\n",
    "    graph_builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "    # Create a MemorySaver for persistence\n",
    "    memory = MemorySaver()\n",
    "    \n",
    "    # Compile the graph with the memory checkpointer and specify interrupt points\n",
    "    sql_graph = graph_builder.compile(checkpointer=memory, interrupt_before=[\"execute_query\"])\n",
    "    \n",
    "    # Return the compiled graph and the configuration\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    \n",
    "    return sql_graph, config\n",
    "\n",
    "def run_sql_generator(graph,config,ques):\n",
    "    # for step in graph.stream(ques, config,stream_mode=\"updates\"):\n",
    "    #     print(step)\n",
    "    # graph.invoke()\n",
    "    graph.invoke(ques, config)\n",
    "\n",
    "\n",
    "    try:\n",
    "        user_approval = 'yes'#input(\"Do you want to go to execute query? (yes/no): \")\n",
    "    except Exception:\n",
    "        user_approval = \"no\"\n",
    "\n",
    "    if user_approval.lower() == \"yes\":\n",
    "        # If approved, continue the graph execution\n",
    "        for step in graph.stream(None, config, stream_mode=\"updates\"):\n",
    "            print(step),\n",
    "        formatted_answer = f\"#### {step['generate_answer']['answer'].replace(',', '\\n- ')}\"\n",
    "        display(Markdown(formatted_answer))     \n",
    "    else:\n",
    "        print(\"Operation cancelled by user.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/A200105820/Documents/Gen AI Use Cases/.conda/lib/python3.12/site-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "/var/folders/52/hjg6373532jdtvy0l3dgzwlh0000gq/T/ipykernel_13488/1690414374.py:8: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m ques5 = {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhich is most frequent  BillingCountry?\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      7\u001b[39m sql_graph, config = create_sql_graph()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mrun_sql_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mques1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mrun_sql_generator\u001b[39m\u001b[34m(graph, config, ques)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_sql_generator\u001b[39m(graph,config,ques):\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# for step in graph.stream(ques, config,stream_mode=\"updates\"):\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m#     print(step)\u001b[39;00m\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# graph.invoke()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mques\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    109\u001b[39m         user_approval = \u001b[33m'\u001b[39m\u001b[33myes\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;66;03m#input(\"Do you want to go to execute query? (yes/no): \")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Gen AI Use Cases/.conda/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2714\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2713\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2714\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2715\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2718\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2719\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Gen AI Use Cases/.conda/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2353\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2347\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2348\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2349\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2350\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2351\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2352\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2353\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2355\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2356\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2357\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2360\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2361\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mwrite_query\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     33\u001b[39m prompt = query_prompt_template.invoke(\n\u001b[32m     34\u001b[39m     {\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdialect\u001b[39m\u001b[33m\"\u001b[39m: db.dialect,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     }\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m llm = my_llm()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m structured_llm = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQueryOutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m result = structured_llm.invoke(prompt)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Gen AI Use Cases/.conda/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1399\u001b[39m, in \u001b[36mBaseChatModel.with_structured_output\u001b[39m\u001b[34m(self, schema, include_raw, **kwargs)\u001b[39m\n\u001b[32m   1396\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mwith_structured_output is not implemented for this model.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1397\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1399\u001b[39m llm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43many\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mls_structured_output_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmethod\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_calling\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mschema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_basemodel_subclass(schema):\n\u001b[32m   1408\u001b[39m     output_parser: OutputParserLike = PydanticToolsParser(\n\u001b[32m   1409\u001b[39m         tools=[cast(\u001b[33m\"\u001b[39m\u001b[33mTypeBaseModel\u001b[39m\u001b[33m\"\u001b[39m, schema)], first_tool_only=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1410\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Gen AI Use Cases/.conda/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1273\u001b[39m, in \u001b[36mBaseChatModel.bind_tools\u001b[39m\u001b[34m(self, tools, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_tools\u001b[39m(\n\u001b[32m   1256\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1257\u001b[39m     tools: Sequence[\n\u001b[32m   (...)\u001b[39m\u001b[32m   1262\u001b[39m     **kwargs: Any,\n\u001b[32m   1263\u001b[39m ) -> Runnable[LanguageModelInput, BaseMessage]:\n\u001b[32m   1264\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Bind tools to the model.\u001b[39;00m\n\u001b[32m   1265\u001b[39m \n\u001b[32m   1266\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m \u001b[33;03m        A Runnable that returns a message.\u001b[39;00m\n\u001b[32m   1272\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: ",
      "During task with name 'write_query' and id 'd24fc1a1-5b73-e137-051a-830b0bf9bca7'"
     ]
    }
   ],
   "source": [
    "ques1 = {\"question\": \"what are the different tables available in database? Shownit as bullet points\"}\n",
    "ques2 = {\"question\": \"can you provide the column names in each column in tabular format like tabel_name,Column_name,Description?\"}\n",
    "ques3 = {'question': \"How many different types of Genre? Provide them with number of times it occus in our Genre table \"}\n",
    "ques4 = {'question': \"How many different types of Genre? Provide them with number of times it occus in our Genre table \"}\n",
    "ques5 = {\"question\": \"How many employees are there?\"}\n",
    "ques5 = {\"question\": \"Which is most frequent  BillingCountry?\"}\n",
    "sql_graph, config = create_sql_graph()\n",
    "run_sql_generator(sql_graph,config,ques1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
